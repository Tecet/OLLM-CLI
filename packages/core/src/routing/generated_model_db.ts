// This file is generated by scripts/generate_model_db.js
// Do not edit by hand.
import type { ModelEntry } from './modelDatabase.js';

export const GENERATED_MODEL_DB: ModelEntry[] = [
  {
    "pattern": "user-unknown-model",
    "family": "user-unknown-model",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": false
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "qwen2.5:7b",
    "family": "qwen2.5:7b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": true,
      "vision": false,
      "streaming": true,
      "reasoning": false
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "deepseek-r1:7b",
    "family": "deepseek-r1:7b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "phi3:3.8b",
    "family": "phi3:3.8b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "llama3.2:3b",
    "family": "llama3.2:3b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": true,
      "vision": false,
      "streaming": true,
      "reasoning": false
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "qwen3:4b",
    "family": "qwen3:4b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": true,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "deepseek-r1:1.5b",
    "family": "deepseek-r1:1.5b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "gemma3:4b",
    "family": "gemma3:4b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": true,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "gemma3:1b",
    "family": "gemma3:1b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": false
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "qwen3-vl:8b",
    "family": "qwen3-vl:8b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": true,
      "vision": true,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  },
  {
    "pattern": "deepseek-r1:8b",
    "family": "deepseek-r1:8b",
    "contextWindow": 131072,
    "capabilities": {
      "toolCalling": false,
      "vision": false,
      "streaming": true,
      "reasoning": true
    },
    "profiles": [
      "4k",
      "8k",
      "16k",
      "32k",
      "64k",
      "128k"
    ]
  }
] as any;

export const GENERATED_RAW_PROFILES = {
  "user-unknown-model": {
    "id": "user-unknown-model",
    "name": "Unknown Model",
    "creator": "User",
    "parameters": "Based on Llama 3.2 3B",
    "quantization": "Based on Llama 3.2 3B (4-bit estimated)",
    "description": "Unknown model - please edit your settings at ~/.ollm/LLM_profiles.json (Windows: C:\\Users\\{username}\\.ollm\\LLM_profiles.json)",
    "abilities": [
      "Unknown"
    ],
    "tool_support": false,
    "ollama_url": "https://github.com/Tecet/OLLM/tree/main/docs",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "2.5 GB",
        "ollama_context_size": 2867,
        "vram_estimate_gb": 2.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "2.9 GB",
        "ollama_context_size": 5734,
        "vram_estimate_gb": 2.9
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "3.7 GB",
        "ollama_context_size": 11468,
        "vram_estimate_gb": 3.7
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "5.2 GB",
        "ollama_context_size": 22937,
        "vram_estimate_gb": 5.2
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "8.2 GB",
        "ollama_context_size": 45875,
        "vram_estimate_gb": 8.2
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "14.2 GB",
        "ollama_context_size": 91750,
        "vram_estimate_gb": 14.2
      }
    ],
    "default_context": 4096
  },
  "qwen2.5:7b": {
    "id": "qwen2.5:7b",
    "name": "Qwen2.5 7B",
    "creator": "Alibaba Cloud",
    "parameters": "7.6B",
    "quantization": "4-bit (estimated)",
    "description": "A comprehensive 7B parameter model from the Qwen2.5 series, offering a balance of performance and efficiency.",
    "abilities": [
      "General Purpose",
      "Coding",
      "Math",
      "Multilingual"
    ],
    "tool_support": true,
    "ollama_url": "https://ollama.com/library/qwen2.5",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "5.5 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 5.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "6.0 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 6
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "7.0 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 7
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "9.0 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 9
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "13.5 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 13.5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "22.5 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 22.5
      }
    ],
    "default_context": 4096
  },
  "deepseek-r1:7b": {
    "id": "deepseek-r1:7b",
    "name": "DeepSeek R1 7B",
    "creator": "DeepSeek",
    "parameters": "7B",
    "quantization": "4-bit (estimated)",
    "description": "A distilled reasoning model based on Qwen2.5-Math-7B, optimized for complex logical tasks.",
    "abilities": [
      "Reasoning",
      "Math",
      "Logic"
    ],
    "tool_support": false,
    "thinking_enabled": true,
    "reasoning_buffer": "Variable",
    "warmup_timeout": 120000,
    "ollama_url": "https://ollama.com/library/deepseek-r1",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "5.5 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 5.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "6.0 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 6
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "7.0 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 7
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "9.0 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 9
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "13.5 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 13.5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "22.5 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 22.5
      }
    ],
    "default_context": 4096
  },
  "phi3:3.8b": {
    "id": "phi3:3.8b",
    "name": "Phi-3 Mini",
    "creator": "Microsoft",
    "parameters": "3.8B",
    "quantization": "4-bit (estimated)",
    "description": "A lightweight, high-performance model suitable for mobile and edge deployments.",
    "abilities": [
      "Reasoning",
      "Coding",
      "Mobile Optimized"
    ],
    "tool_support": false,
    "ollama_url": "https://ollama.com/library/phi3",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "2.8 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 2.8
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "3.2 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 3.2
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "4.0 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 4
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "5.5 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 5.5
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "8.5 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 8.5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "14.5 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 14.5
      }
    ],
    "default_context": 4096
  },
  "llama3.2:3b": {
    "id": "llama3.2:3b",
    "name": "Llama 3.2 3B",
    "creator": "Meta",
    "parameters": "3.2B",
    "quantization": "4-bit (estimated)",
    "description": "A highly efficient 3B model from the Llama 3.2 family, designed for edge devices.",
    "abilities": [
      "General Purpose",
      "Fast Inference",
      "Edge Optimized"
    ],
    "tool_support": true,
    "ollama_url": "https://ollama.com/library/llama3.2",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "2.5 GB",
        "ollama_context_size": 2867,
        "vram_estimate_gb": 2.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "2.9 GB",
        "ollama_context_size": 5734,
        "vram_estimate_gb": 2.9
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "3.7 GB",
        "ollama_context_size": 11468,
        "vram_estimate_gb": 3.7
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "5.2 GB",
        "ollama_context_size": 22937,
        "vram_estimate_gb": 5.2
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "8.2 GB",
        "ollama_context_size": 45875,
        "vram_estimate_gb": 8.2
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "14.2 GB",
        "ollama_context_size": 91750,
        "vram_estimate_gb": 14.2
      }
    ],
    "default_context": 4096
  },
  "qwen3:4b": {
    "id": "qwen3:4b",
    "name": "Qwen3 4B",
    "creator": "Alibaba Cloud",
    "parameters": "4B",
    "quantization": "4-bit (estimated)",
    "description": "Next-generation compact model from the Qwen series, delivering superior performance for its size.",
    "abilities": [
      "General Purpose",
      "Coding",
      "Advanced Reasoning"
    ],
    "tool_support": true,
    "thinking_enabled": true,
    "ollama_url": "https://ollama.com/library/qwen3",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "3.0 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 3
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "3.5 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 3.5
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "4.5 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 4.5
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "6.0 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 6
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "9.5 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 9.5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "16.0 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 16
      }
    ],
    "default_context": 4096
  },
  "deepseek-r1:1.5b": {
    "id": "deepseek-r1:1.5b",
    "name": "DeepSeek R1 1.5B",
    "creator": "DeepSeek",
    "parameters": "1.5B",
    "quantization": "4-bit (estimated)",
    "description": "Ultra-lightweight distilled reasoning model, perfect for extremely low-resource environments.",
    "abilities": [
      "Reasoning",
      "Math",
      "Efficiency"
    ],
    "tool_support": false,
    "thinking_enabled": true,
    "reasoning_buffer": "Variable",
    "warmup_timeout": 120000,
    "ollama_url": "https://ollama.com/library/deepseek-r1",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "1.5 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 1.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "1.8 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 1.8
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "2.4 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 2.4
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "3.5 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 3.5
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "5.8 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 5.8
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "10.0 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 10
      }
    ],
    "default_context": 4096
  },
  "gemma3:4b": {
    "id": "gemma3:4b",
    "name": "Gemma 3 4B",
    "creator": "Google",
    "parameters": "4B",
    "quantization": "4-bit (estimated)",
    "description": "Google's latest open model, optimized for efficiency and performance on consumer hardware.",
    "abilities": [
      "General Purpose",
      "Knowledge Retrieval",
      "Reasoning"
    ],
    "tool_support": true,
    "ollama_url": "https://ollama.com/library/gemma3",
    "max_context_window": 131072,
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "3.8 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 3.8
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "4.3 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 4.3
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "5.3 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 5.3
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "7.2 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 7.2
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "11.0 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 11
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "18.5 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 18.5
      }
    ],
    "default_context": 4096
  },
  "gemma3:1b": {
    "id": "gemma3:1b",
    "name": "Gemma 3 1B",
    "creator": "Google",
    "parameters": "1B",
    "quantization": "4-bit (estimated)",
    "description": "Extremely compact variant of Gemma 3, ideal for simple tasks and rapid prototyping.",
    "abilities": [
      "Fast Inference",
      "Text Completion"
    ],
    "tool_support": false,
    "ollama_url": "https://ollama.com/library/gemma3",
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "1.2 GB",
        "ollama_context_size": 2867,
        "vram_estimate_gb": 1.2
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "1.5 GB",
        "ollama_context_size": 5734,
        "vram_estimate_gb": 1.5
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "2.0 GB",
        "ollama_context_size": 11468,
        "vram_estimate_gb": 2
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "3.0 GB",
        "ollama_context_size": 22937,
        "vram_estimate_gb": 3
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "5.0 GB",
        "ollama_context_size": 45875,
        "vram_estimate_gb": 5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "9.0 GB",
        "ollama_context_size": 91750,
        "vram_estimate_gb": 9
      }
    ],
    "default_context": 4096,
    "max_context_window": 131072
  },
  "qwen3-vl:8b": {
    "id": "qwen3-vl:8b",
    "name": "Qwen3-VL 8B",
    "creator": "Alibaba Cloud",
    "parameters": "8B",
    "quantization": "4-bit (estimated)",
    "description": "Vision-Language model from the Qwen3 series, capable of understanding and analyzing images.",
    "abilities": [
      "Visual Recognition",
      "Multimodal",
      "Reasoning"
    ],
    "tool_support": true,
    "ollama_url": "https://ollama.com/library/qwen3-vl",
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "6.5 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 6.5
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "7.0 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 7
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "8.0 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 8
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "10.0 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 10
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "15.0 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 15
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "26.0 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 26
      }
    ],
    "default_context": 4096,
    "max_context_window": 131072
  },
  "deepseek-r1:8b": {
    "id": "deepseek-r1:8b",
    "name": "DeepSeek R1 8B",
    "creator": "DeepSeek",
    "parameters": "8B",
    "quantization": "4-bit (estimated)",
    "description": "Powerful reasoning model with enhanced capacity for complex problem solving.",
    "abilities": [
      "Advanced Reasoning",
      "Coding",
      "Math"
    ],
    "tool_support": false,
    "thinking_enabled": true,
    "reasoning_buffer": "Variable",
    "warmup_timeout": 120000,
    "ollama_url": "https://ollama.com/library/deepseek-r1",
    "context_profiles": [
      {
        "size": 4096,
        "size_label": "4k",
        "vram_estimate": "6.0 GB",
        "ollama_context_size": 3482,
        "vram_estimate_gb": 6
      },
      {
        "size": 8192,
        "size_label": "8k",
        "vram_estimate": "6.5 GB",
        "ollama_context_size": 6963,
        "vram_estimate_gb": 6.5
      },
      {
        "size": 16384,
        "size_label": "16k",
        "vram_estimate": "7.5 GB",
        "ollama_context_size": 13926,
        "vram_estimate_gb": 7.5
      },
      {
        "size": 32768,
        "size_label": "32k",
        "vram_estimate": "9.5 GB",
        "ollama_context_size": 27853,
        "vram_estimate_gb": 9.5
      },
      {
        "size": 65536,
        "size_label": "64k",
        "vram_estimate": "14.5 GB",
        "ollama_context_size": 55706,
        "vram_estimate_gb": 14.5
      },
      {
        "size": 131072,
        "size_label": "128k",
        "vram_estimate": "25.0 GB",
        "ollama_context_size": 111411,
        "vram_estimate_gb": 25
      }
    ],
    "default_context": 4096,
    "max_context_window": 131072
  }
} as any;
